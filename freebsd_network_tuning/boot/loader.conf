# FreeBSD 11 -- /boot/loader.conf  version 0.55
# https://calomel.org/freebsd_network_tuning.html
#
# low latency is important so we highly recommend that you disable hyper
# threading on Intel CPUs as it has an unpredictable affect on latency, cpu
# cache misses and load. 
#
# These settings are specifically tuned for a low latency one(1) gigabit and
# gigabit LAN connections. If you have 10gig or 40gig you will need to increase
# the network buffers as proposed.

# ZFS root boot config
zfs_load="YES"
vfs.root.mountfrom="zfs:zroot"

# NVMe kernel modules, preload
#nvme_load="YES"
#nvd_load="YES"

# Pf firewall kernel modules, preload
pf_load="YES"
pflog_load="YES"

# Intel igb(4) kernel driver, preload
if_igb_load="YES"

# Advanced Host Controller Interface (AHCI) 
ahci_load="YES"

# H-TCP Congestion Control for a more aggressive increase in speed on higher
# latency, high bandwidth networks with some packet loss. 
cc_htcp_load="YES"

# How many seconds to sit at the boot menu before booting the server. Reduce
# this value for a faster booting machine or set to "-1" for no delay. For a
# server, you may want to increase this time if you have the BIOS auto boot
# after a power outage or brownout. By increasing the delay you allow more time
# for the power grid to stabilize and UPS batteries to re-charge. Ideally, you
# want to avoid the system fast booting into the OS and mounting the file
# system only to power off due to another brownout. If you are at the console
# during boot you can always hit enter to bypass this delay.
autoboot_delay="-1"  # (default 10) seconds

# ZFS: the maximum upper limit of RAM used for dirty, "modified", uncommitted
# data which vfs.zfs.dirty_data_max can not exceed. The server has 32GB of RAM
# in which we will allow up to 12GB, if needed, to cache incoming data before
# TXG commit to the PCIe NVMe array. Note: the dirty_data cache is part of the
# Adaptive Replacement Cache (ARC) and can be viewed in "top" as the "Anon"
# value under ARC.
vfs.zfs.dirty_data_max_max="12884901888" # (default 4294967296, 4GB)

# hostcache cachelimit is the number of ip addresses in the hostcache list.
# Setting the value to zero(0) stops any ip address connection information from
# being cached and negates the need for "net.inet.tcp.hostcache.expire". We
# find disabling the hostcache increases burst data rates by 2x if a subnet was
# incorrectly graded as slow on a previous connection. A host cache entry is
# the client's cached tcp connection details and metrics (TTL, SSTRESH and
# VARTTL) the server can use to improve future performance of connections
# between the same two hosts. When a tcp connection is completed, our server
# will cache information about the connection until an expire timeout. If a new
# connection between the same client is initiated before the cache has expired,
# the connection will use the cached connection details to setup the
# connection's internal variables. This pre-cached setup allows the client and
# server to reach optimal performance significantly faster because the server
# will not need to go through the usual steps of re-learning the optimal
# parameters for the connection. To view the current host cache stats use
# "sysctl net.inet.tcp.hostcache.list" 
net.inet.tcp.hostcache.cachelimit="0"

# Change the zfs pool output to show the GPT id for each drive instead of the
# gptid or disk identifier. The gpt id will look like "ada0p2"; the gpt id
# "ada0" of the first drive found on the AHCI SATA / SAS / SCSI chain and
# partition string "p2".  Use "gpart list" to see all drive identifiers and
# "zpool status" to see the chosen id through ZFS.
kern.geom.label.disk_ident.enable="0"
kern.geom.label.gpt.enable="1"
kern.geom.label.gptid.enable="0"

# Interface Maximum Queue Length: common recommendations are to set the interface
# buffer size to the number of packets the interface can transmit (send) in 50
# milliseconds _OR_ 256 packets times the number of interfaces in the machine;
# whichever value is greater. To calculate a size of a 50 millisecond buffer
# for a 60 megabit network take the bandwidth in megabits divided by 8 bits
# divided by the MTU times 50 millisecond times 1000, 60/8/1460*50*1000=256.84
# packets in 50 milliseconds. OR, if the box has two(2) interfaces take 256
# packets times two(2) NICs to equal 512 packets. 512 is greater then 256.84 so
# set to 512.
#
# Our preference is to define the interface queue length as two(2) times the
# value set in the interface transmit descriptor ring, "hw.igb.txd". If
# hw.igb.txd="1024" then set the net.link.ifqmaxlen="2048". 
#
# An indirect result of increasing the interface queue is the buffer acts like
# a large TCP initial congestion window (init_cwnd) by allowing a network stack
# to burst packets at the start of a connection. Do not to set to zero(0) or
# the network will stop working due to "no network buffers" available. Do not
# set the interface buffer ludicrously large to avoid buffer bloat.
net.link.ifqmaxlen="2048"  # (default 50)

# Enable the optimized version of soreceive() for stream (TCP) sockets.
# soreceive_stream() only does one sockbuf unlock/lock per receive independent
# of the length of data to be moved into the uio compared to soreceive() which
# unlocks/locks per *mbuf*. soreceive_stream() can significantly reduced CPU
# usage and lock contention when receiving fast TCP streams. Additional gains
# are obtained when the receiving application is using SO_RCVLOWAT to batch up
# some data before a read (and wakeup) is done. 
net.inet.tcp.soreceive_stream="1"  # (default 0)

# Intel igb(4): FreeBSD puts an upper limit on the the number of received
# packets a network card can process to 100 packets per interrupt cycle. This
# limit is in place because of inefficiencies in IRQ sharing when the network
# card is using the same IRQ as another device. When the Intel network card is
# assigned a unique IRQ (dmesg) and MSI-X is enabled through the driver
# (hw.igb.enable_msix=1) then interrupt scheduling is hundreds of times more
# efficient and the NIC can be allowed to process packets as fast as they are
# received. A value of "-1" means unlimited packet processing and sets the same
# value to dev.igb.0.rx_processing_limit and dev.igb.1.rx_processing_limit .
hw.igb.rx_process_limit="-1"  # (default 100)

# Intel igb(4): The Intel i350-T2 dual port NIC supports up to eight(8)
# input/output queues per network port, the card has two(2) network ports.
#
# Multiple transmit and receive queues in network hardware allow network
# traffic streams to be distributed into queues. Queues can be mapped by the
# FreeBSD network card driver to specific processor cores leading to reduced
# CPU cache misses. Queues also distribute the workload over multiple CPU
# cores, process network traffic in parallel and prevent network traffic or
# interrupt processing from overwhelming a single CPU core.
#
# http://www.intel.com/content/dam/doc/white-paper/improving-network-performance-in-multi-core-systems-paper.pdf
# 
# For a firewall, we recommend setting the number of network queues equal to
# the total number of real CPU cores in the machine divided by the number of
# active network ports. For example, a firewall with four(4) real CPU cores and
# an i350-T2 dual port NIC should use two(2) queues per network port
# (hw.igb.num_queues=2). This equals a total of four(4) network queues over
# two(2) network ports which map to to four(4) real CPU cores. A FreeBSD server
# with four(4) real CPU cores and a single network port should use four(4)
# network queues. Simply set hw.igb.num_queues to zero(0) to allow the FreeBSD
# driver to automatically set the number of network queues to the number of CPU
# cores. It is not recommend to allow more network queues than real CPU cores
# per network port.
#
# Query total interrupts per queue with "vmstat -i" and use "top -H -S" to
# watch CPU usage per igb0:que. Multiple network queues will trigger more total
# interrupts compared to a single network queue, but the processing of each of
# those queues will be spread over multiple CPU cores allowing the system to
# handle increased network traffic loads.
hw.igb.num_queues="2"  # (default 0 , queues equal the number of CPU real cores)

###
######
######### OFF BELOW HERE #########
#
# Other options not used, but included for future reference.

# accf accept filters are used so the server will not have to context switch
# several times before performing the initial parsing of the request. This
# could decrease server load by reducing the amount of CPU time to handle
# incoming requests.  buffer incoming connections until complete HTTP requests
# arrive (nginx apache) for nginx http add, "listen 127.0.0.1:80
# accept_filter=httpready;"
#accf_http_load="YES"

# FreeBSD kernel accept filter for non-http daemons, like https (ssl) proxies,
# web servers and ssl accelerators. A local daemon can request the FreeBSD
# kernel to buffer incoming client connections until the remote client sends
# data. The daemon benefits from the accf_data filter by offloading the initial
# connection processing and data stream check to the FreeBSD kernel. It is only
# when the remote client starts to send data, does the data filter release the
# data stream to the accept() system call the local daemon is listening on.
# Since the web server daemon focuses soley on active connections, the system
# will scale up to serve a greater number of clients. Apache, Nginx and
# Varnish support the accept data filter. For nginx https servers add "listen
# 127.0.0.1:443 ssl accept_filter=dataready;" to the nginx.conf
#accf_data_load="YES"

# Asynchronous I/O, or non-blocking I/O is a form of input/output processing
# permitting other processing to continue before the transmission has finished.
# AIO is used for accelerating Nginx on ZFS. Check for our tutorials on both.
# FreeBSD 11.0 removed the aio kernel module
#aio_load="YES"

# qlimit for igmp, arp, ether and ip6 queues only (netstat -Q) (default 256)
#net.isr.defaultqlimit="2048" # (default 256)

# enable /dev/crypto for IPSEC of custom seeding using the AES-NI Intel
# hardware cpu support
#aesni_load="YES"

# load the Intel PRO/1000 PCI Express kernel module on boot
#if_em_load="YES"

# load the Myri10GE kernel module on boot
#if_mxge_load="YES"

# load the PF CARP module
#if_carp_load="YES"

# Wait for full DNS request accept filter (unbound)
#accf_dns_load="YES"

# CUBIC Congestion Control allows for more fairness between flows since the
# window growth is independent of RTT.
#cc_cubic_load="YES"

######################################### intel igb tuning ##############

# Intel igb(4): netmap is natively supported on the following network devices
# on FreeBSD: em(4), igb(4), ixgbe(4), lem(4), re(4)

# Once of the best upgrades for a network server is to replace the network
# interface with an efficient network card. The on-board chipsets use a
# significant amount of CPU time. By simply installing an Intel i350 network
# card you can reduce CPU time and interrupt processing and reduce latency.

# Intel igb(4): Message Signaled Interrupts (MSI-X) provide multiple interrupt
# vectors, which allow multiple interrupts to be handled simultaneously and
# loadbalanced across multiple cores. This improvement helps improve CPU
# utilization and lowers latency.
#
# Verify MSI-X is being used by the NIC using "dmesg | grep -i msi" with the
# output looking similar to, "igb0: Using MSIX interrupts with 5 vectors" for a
# two(2) port, four(4) queue Intel i350-T2 network card.
#hw.igb.enable_msix="1"  # (default 1)

# Intel igb(4): Adaptive interrupt Moderation adjusts the interrupt rate
# dynamically based on packet size and throughput and reduces system load for
# igb(4). Enabling AIM, and the separate MSIX option, will result in
# significantly better efficiency in the network stack.
#hw.igb.enable_aim="1"  # (default 1)

# Intel igb(4): Intel PRO 1000 network chipsets support a maximum of 4096 Rx
# and 4096 Tx descriptors. Two cases when you could change the amount of
# descriptors are: 1) Low RAM and 2) CPU or bus saturation. If the system RAM
# is too low you can drop the amount of descriptors to 128, but the system may
# drop packets if it can not processes the packets fast enough. If you have a
# large number of packets incoming and they are being processed too slowly then
# you can increase to the descriptors up to 4096. Increasing descriptors is
# only a hack because the system is too slow to processes the packets in a
# timely manner. You should look into getting a faster CPU with a wider bus or
# identifying why the receiving application is so slow. Use "netstat -ihw 1"
# and look for idrops. Note that each received packet requires one Receive
# Descriptor, and each descriptor uses 2 KB of memory.
# https://fasterdata.es.net/host-tuning/nic-tuning/
#hw.igb.rxd="2048"  # (default 1024)
#hw.igb.txd="2048"  # (default 1024)

# maximum number of interrupts per second generated by single igb(4) (default
# 8000). FreeBSD 10 supports the new drivers which reduces interrupts
# significantly.
#hw.igb.max_interrupt_rate="32000" # (default 8000)

# Intel igb(4): using older intel drivers and jumbo frames caused memory
# fragmentation as header splitting wouldn't allocate jumbo clusters. The
# current intel drivers do not seem to have these issues, so headers splitting
# is disabled by default.
#hw.igb.header_split=0 # (default 0)

######################################### intel igb tuning ##############

# thermal sensors for intel or amd cpus
#coretemp_load="YES"
#amdtemp_load="YES"

# higher HZ settings have a negative impact on machine performance due to
# handling more timer interrupts resulting in more context switches and cache
# flushes (default 1000).  Lower HZ settings can have a detrimental effect on
# ZFS.
# http://lists.freebsd.org/pipermail/freebsd-questions/2005-April/083482.html
# Also take a look into kern.sched.interact and kern.sched.slice in
# /etc/sysctl.conf
#kern.hz=1000

# increase the number of network mbufs the system is willing to allocate.  Each
# cluster represents approximately 2K of memory, so a value of 524288
# represents 1GB of kernel memory reserved for network buffers. (default
# 492680)
#kern.ipc.nmbclusters="492680"
#kern.ipc.nmbjumbop="246339"

# maximum number of interrupts per second on any interrupt level (vmstat -i for
# total rate). If you still see Interrupt Storm detected messages, increase the
# limit to a higher number and look for the culprit.  For 10gig NIC's set to
# 9000 and use large MTU. (default 1000)
#hw.intr_storm_threshold="9000"

# Size of the syncache hash table, must be a power of 2 (default 512)
#net.inet.tcp.syncache.hashsize="1024"

# Limit the number of entries permitted in each bucket of the hash table. (default 30)
#net.inet.tcp.syncache.bucketlimit="100"

# number of hash table buckets to handle incoming tcp connections. a value of
# 65536 allows the system to handle millions incoming connections. each tcp
# entry in the hash table on x86_64 uses 252 bytes of ram.  vmstat -z | egrep
# "ITEM|tcpcb" (default 65536 which is ~16M connections)
#net.inet.tcp.tcbhashsize="65536"

# when booting, display the ascii art FreeBSD Orb with the two horns on top.
# Just a cosmetic preference over "beastie", the multicolored daemon with
# pitchfork and oversized shoes.
#loader_logo="orb"

######################################### net.isr. tuning begin ##############
# NOTE regarding "net.isr.*" : Processor affinity can effectively reduce cache
# problems but it does not curb the persistent load-balancing problem.[1]
# Processor affinity becomes more complicated in systems with non-uniform
# architectures. A system with two dual-core hyper-threaded CPUs presents a
# challenge to a scheduling algorithm. There is complete affinity between two
# virtual CPUs implemented on the same core via hyper-threading, partial
# affinity between two cores on the same physical chip (as the cores share
# some, but not all, cache), and no affinity between separate physical chips.
# It is possible that net.isr.bindthreads="0" and net.isr.maxthreads="3" can
# cause more slowdown if your system is not cpu loaded already. We highly
# recommend getting a more efficient network card instead of setting the
# "net.isr.*" options. Look at the Intel i350 for gigabit or the Myricom
# 10G-PCIE2-8C2-2S for 10gig. These cards will reduce the machines nic
# processing to 12% or lower.

# For high bandwidth systems setting bindthreads to "0" will spread the
# network processing load over multiple cpus allowing the system to handle more
# throughput. The default is faster for most lightly loaded systems (default 0)
#net.isr.bindthreads="0"

# qlimit for igmp, arp, ether and ip6 queues only (netstat -Q) (default 256)
#net.isr.defaultqlimit="256"

# interrupt handling via multiple CPU (default direct)
#net.isr.dispatch="direct"

# limit per-workstream queues (use "netstat -Q" if Qdrop is greater then 0
# increase this directive) (default 10240)
#net.isr.maxqlimit="10240"

# Max number of threads for NIC IRQ balancing. Set to the number of real cpu
# cores in the box. maxthreads will spread the load over multiple cpu cores at
# the the cost of cpu affinity unbinding. The default of "1" should be faster if
# the machine is mostly idle.
#net.isr.maxthreads="1"

############################################ zfs tuning begin ##############
# martin_matuska_eurobsdcon_2012 = http://www.youtube.com/watch?v=PIpI7Ub6yjo
#
# The goal is to keep as much data in RAM before committing to maximize long,
# concurrent writes and reduce data fragmentation. This machine has eight(8)
# gigabytes of RAM with zfs mirrored 4 TB drives.

# Dynamically adjust max write limit based on previous txg commits to attempt
# to maintain a 3-second commit time. If the SATA based mirrored pool can write
# at 120 MB/sec then the goal is to keep at least (120 MB/sec times 3 seconds
# equals) 360 MB of data in the write cache to be written to the pool all at
# once.
#vfs.zfs.txg.synctime_ms="3000"  # default 1000

# Commit async writes if the maximum I/O requests pending on each device reach
# the limit.
#vfs.zfs.vdev.max_pending="32"  # default 10

# Commit async writes after 120 seconds if the max write limit is not reached.
# WARNING: in the worst case scenario we could loose all 120 seconds worth of
# data if the machine is abruptly powered off or looses power.
#vfs.zfs.txg.timeout="120"  # default 5 seconds

# Increase VDEV read ahead cache size. This reduces scrub times and
# metadata-intensive tasks for a small cost in RAM. The vdev cache uses a
# simple FIFO rolling data set.
#vfs.zfs.vdev.cache.size="64M"
#vfs.zfs.vdev.cache.max="65536" 

# Default vfs.zfs.write_limit_shift appears to be "3" which on a system
# with 2GB RAM such as this one results in a write_limit_max of 256MB, which
# is appropriate, so we're not going to change that here.
#vfs.zfs.write_limit_shift="3"

############################################# zfs tuning end ###############

# SIFTR (Statistical Information For TCP Research) is a kernel module which
# logs a range of statistics on active TCP connections to a log file in comma
# separated format. Only useful for researching tcp flows as it does add some
# processing load to the system.
# http://manpages.ubuntu.com/manpages/precise/man4/siftr.4freebsd.html
#siftr_load="YES"

#
##
### EOF ###
